{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.546714Z",
     "start_time": "2025-01-14T09:23:34.512899Z"
    }
   },
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.570017Z",
     "start_time": "2025-01-14T09:23:38.557001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name)"
   ],
   "id": "be4d1a8cb52d9e48",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.586219Z",
     "start_time": "2025-01-14T09:23:38.579821Z"
    }
   },
   "cell_type": "code",
   "source": "environment_name",
   "id": "341d0ddf1b85d90a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CartPole-v1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.802294Z",
     "start_time": "2025-01-14T09:23:38.792138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes + 1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n",
    "env.close()\n"
   ],
   "id": "ad2b6a248723e362",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:22.0\n",
      "Episode:2 Score:15.0\n",
      "Episode:3 Score:27.0\n",
      "Episode:4 Score:23.0\n",
      "Episode:5 Score:14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\pkg\\envs\\VisualIntervention\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:211: UserWarning: \u001B[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001B[0m\n",
      "  gym.logger.warn(\n",
      "D:\\Software\\Anaconda\\pkg\\envs\\VisualIntervention\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.819452Z",
     "start_time": "2025-01-14T09:23:38.811442Z"
    }
   },
   "cell_type": "code",
   "source": "env.reset()",
   "id": "220b0e241901e2c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01750497,  0.02629052, -0.03772727, -0.01677893], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.852337Z",
     "start_time": "2025-01-14T09:23:38.837105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    print(episode)"
   ],
   "id": "e3b6158680e56f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.888571Z",
     "start_time": "2025-01-14T09:23:38.878433Z"
    }
   },
   "cell_type": "code",
   "source": "env.reset()",
   "id": "3f5d669847422e20",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.0282611 , -0.03701731,  0.01570831, -0.04062297], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.922891Z",
     "start_time": "2025-01-14T09:23:38.915356Z"
    }
   },
   "cell_type": "code",
   "source": "env.observation_space",
   "id": "673e946a35c2ef86",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.960806Z",
     "start_time": "2025-01-14T09:23:38.952328Z"
    }
   },
   "cell_type": "code",
   "source": "env.step(1)",
   "id": "f52321a2f35465ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02900145,  0.15787591,  0.01489586, -0.32830867], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:38.995552Z",
     "start_time": "2025-01-14T09:23:38.978207Z"
    }
   },
   "cell_type": "code",
   "source": "env.observation_space.sample()",
   "id": "1c4cb7ef55813438",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.7848816e+00,  1.5430731e+38, -3.6797978e-02,  1.4462641e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:39.029538Z",
     "start_time": "2025-01-14T09:23:39.017732Z"
    }
   },
   "cell_type": "code",
   "source": "env.action_space.sample()",
   "id": "49de2c207659348a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:39.064320Z",
     "start_time": "2025-01-14T09:23:39.059711Z"
    }
   },
   "cell_type": "code",
   "source": "env.action_space",
   "id": "645773e667e92a64",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:39.105279Z",
     "start_time": "2025-01-14T09:23:39.100567Z"
    }
   },
   "cell_type": "code",
   "source": "env.observation_space.sample()",
   "id": "846ac7ada449f9f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.5666826e+00, -1.5152727e+38, -1.0846296e-01,  5.2979540e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:39.163688Z",
     "start_time": "2025-01-14T09:23:39.157814Z"
    }
   },
   "cell_type": "code",
   "source": "env.action_space.sample()",
   "id": "baa2340fa7a1085d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:39.191444Z",
     "start_time": "2025-01-14T09:23:39.184945Z"
    }
   },
   "cell_type": "code",
   "source": " log_path = os.path.join('Training', 'Logs')",
   "id": "af00066c43c81774",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:39.203271Z",
     "start_time": "2025-01-14T09:23:39.197875Z"
    }
   },
   "cell_type": "code",
   "source": " log_path",
   "id": "ec611f92ddc83477",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:42.973726Z",
     "start_time": "2025-01-14T09:23:39.230829Z"
    }
   },
   "cell_type": "code",
   "source": [
    " env = gym.make(environment_name)\n",
    " env = DummyVecEnv([lambda: env])\n",
    " model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ],
   "id": "18dac4210d48a644",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\pkg\\envs\\VisualIntervention\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\pkg\\envs\\VisualIntervention\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:23:43.070691Z",
     "start_time": "2025-01-14T09:23:42.988297Z"
    }
   },
   "cell_type": "code",
   "source": "PPO??",
   "id": "126a3a2df964289",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:26:04.737507Z",
     "start_time": "2025-01-14T09:23:43.179542Z"
    }
   },
   "cell_type": "code",
   "source": "model.learn(total_timesteps=20000)",
   "id": "15b626dc7e9a6c8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\pkg\\envs\\VisualIntervention\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 599  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007993591 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.00264     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.51        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 51.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 298         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010238928 |\n",
      "|    clip_fraction        | 0.0796      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.0834      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 35.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 249        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 32         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01102016 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.635     |\n",
      "|    explained_variance   | 0.26       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.2       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    value_loss           | 49.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008886607 |\n",
      "|    clip_fraction        | 0.084       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 61.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068667396 |\n",
      "|    clip_fraction        | 0.0477       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.59        |\n",
      "|    explained_variance   | 0.31         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.8         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 66           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053432593 |\n",
      "|    clip_fraction        | 0.0372       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.575       |\n",
      "|    explained_variance   | 0.488        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26           |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00922     |\n",
      "|    value_loss           | 60.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008459176 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.61        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 33.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 120          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058748703 |\n",
      "|    clip_fraction        | 0.075        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.591       |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.9          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    value_loss           | 39.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008950001 |\n",
      "|    clip_fraction        | 0.0618      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0078     |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1c2188ebf20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:26:05.017342Z",
     "start_time": "2025-01-14T09:26:05.005808Z"
    }
   },
   "cell_type": "code",
   "source": "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')",
   "id": "7b7a8f1521e7edc7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:26:05.063015Z",
     "start_time": "2025-01-14T09:26:05.036584Z"
    }
   },
   "cell_type": "code",
   "source": "model.save(PPO_Path)",
   "id": "3412b90c7aa5058e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:26:05.089412Z",
     "start_time": "2025-01-14T09:26:05.082192Z"
    }
   },
   "cell_type": "code",
   "source": "del model",
   "id": "6f5b5a5cbb06230a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:26:05.109784Z",
     "start_time": "2025-01-14T09:26:05.098412Z"
    }
   },
   "cell_type": "code",
   "source": "PPO_Path",
   "id": "d94f64f5370565e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models\\\\PPO_Model_Cartpole'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:26:05.149629Z",
     "start_time": "2025-01-14T09:26:05.117971Z"
    }
   },
   "cell_type": "code",
   "source": "model = PPO.load(PPO_Path, env = env)",
   "id": "73f72a00fa777ca",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:26:25.826272Z",
     "start_time": "2025-01-14T09:26:05.156369Z"
    }
   },
   "cell_type": "code",
   "source": "model.learn(total_timesteps=1000)",
   "id": "a0c2cf7fe5c99c8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_6\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 155  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1c228528590>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:26:51.360080Z",
     "start_time": "2025-01-14T09:26:25.843472Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_policy(model, env, n_eval_episodes=10, render=True)",
   "id": "a604a79dd05f791a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\pkg\\envs\\VisualIntervention\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "D:\\Software\\Anaconda\\pkg\\envs\\VisualIntervention\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:26:51.377992Z",
     "start_time": "2025-01-14T09:26:51.373934Z"
    }
   },
   "cell_type": "code",
   "source": "env.close()",
   "id": "3d98d9eaeb297a53",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:27:00.507366Z",
     "start_time": "2025-01-14T09:26:51.402853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n",
    "#env.close()"
   ],
   "id": "a410385fced775e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[500.]\n",
      "Episode:2 Score:[319.]\n",
      "Episode:3 Score:[500.]\n",
      "Episode:4 Score:[414.]\n",
      "Episode:5 Score:[500.]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:27:00.525969Z",
     "start_time": "2025-01-14T09:27:00.522372Z"
    }
   },
   "cell_type": "code",
   "source": "env.close()",
   "id": "2a84a79c2478bf97",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:27:00.545289Z",
     "start_time": "2025-01-14T09:27:00.532020Z"
    }
   },
   "cell_type": "code",
   "source": "obs = env.reset()",
   "id": "312ac770c0d3dc61",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:27:00.575719Z",
     "start_time": "2025-01-14T09:27:00.562581Z"
    }
   },
   "cell_type": "code",
   "source": "model.predict(obs)",
   "id": "ad822eb9aafc04fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1], dtype=int64), None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:27:00.608898Z",
     "start_time": "2025-01-14T09:27:00.593415Z"
    }
   },
   "cell_type": "code",
   "source": "env.action_space.sample()",
   "id": "afd3885342366cf2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:27:00.639527Z",
     "start_time": "2025-01-14T09:27:00.624511Z"
    }
   },
   "cell_type": "code",
   "source": "env.step(action)",
   "id": "6401b462e96ed536",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3.4712858e-02,  1.9512530e-01,  2.3613403e-04, -3.3370662e-01]],\n",
       "       dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([False]),\n",
       " [{'TimeLimit.truncated': False}])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:27:00.663320Z",
     "start_time": "2025-01-14T09:27:00.655611Z"
    }
   },
   "cell_type": "code",
   "source": "training_log_path = os.path.join(log_path, \"PPO_2\")",
   "id": "ce52da30a05217bd",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:27:00.693082Z",
     "start_time": "2025-01-14T09:27:00.686339Z"
    }
   },
   "cell_type": "code",
   "source": "training_log_path",
   "id": "e8695b2ce59ea66a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs\\\\PPO_2'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-14T09:27:00.713060Z"
    }
   },
   "cell_type": "code",
   "source": "!tensorboard --logdir={training_log_path}",
   "id": "35b4db746760ce8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:02:02.113971Z",
     "start_time": "2025-01-12T10:02:02.108628Z"
    }
   },
   "cell_type": "code",
   "source": "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, EvalCallback",
   "id": "6c69e96696184e8",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:03:29.733836Z",
     "start_time": "2025-01-12T10:03:29.727905Z"
    }
   },
   "cell_type": "code",
   "source": "save_path = os.path.join('Training', 'Saved Models')",
   "id": "d12b5fefdde13f1a",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:03:30.910334Z",
     "start_time": "2025-01-12T10:03:30.902334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(env,\n",
    "                             callback_on_new_best=stop_callback,\n",
    "                             eval_freq=10000,\n",
    "                             best_model_save_path=save_path,verbose=1)"
   ],
   "id": "38869a1515946369",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:04:47.593051Z",
     "start_time": "2025-01-12T10:04:47.197872Z"
    }
   },
   "cell_type": "code",
   "source": "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=training_log_path)",
   "id": "3c597c932809c5b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:07:21.795978Z",
     "start_time": "2025-01-12T10:06:19.717528Z"
    }
   },
   "cell_type": "code",
   "source": "model.learn(total_timesteps=20000, callback=eval_callback)",
   "id": "9e26e64f9a809be1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_2\\PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 76   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 26   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008251371 |\n",
      "|    clip_fraction        | 0.0866      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.631      |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 53.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5387, episode_reward=417.80 +/- 118.39\n",
      "Episode length: 417.80 +/- 118.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 418         |\n",
      "|    mean_reward          | 418         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5387        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008414712 |\n",
      "|    clip_fraction        | 0.0664      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 59          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 417.80  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2494e7df2f0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:08:18.895922Z",
     "start_time": "2025-01-12T10:08:18.888556Z"
    }
   },
   "cell_type": "code",
   "source": "new_arch = [dict(pi=[128,128,128,128], vf=[128,128,128,128])]",
   "id": "93ea37fe0c7cbb06",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:09:47.620367Z",
     "start_time": "2025-01-12T10:09:47.531224Z"
    }
   },
   "cell_type": "code",
   "source": "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch': new_arch})\n",
   "id": "c3675b71a3103e4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\pkg\\envs\\VisualIntervention\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:13:30.220047Z",
     "start_time": "2025-01-12T10:10:52.368004Z"
    }
   },
   "cell_type": "code",
   "source": "model.learn(total_timesteps=20000, callback=eval_callback)",
   "id": "fc0dbddcc27d1658",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 99   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 20   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015257328 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.00573    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.11        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 19.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018216591 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    value_loss           | 26.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118696615 |\n",
      "|    clip_fraction        | 0.181        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.596       |\n",
      "|    explained_variance   | 0.43         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.9         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    value_loss           | 43.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=297.20 +/- 82.92\n",
      "Episode length: 297.20 +/- 82.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 297         |\n",
      "|    mean_reward          | 297         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010078417 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.32        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 31.5        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 124   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 82    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076031196 |\n",
      "|    clip_fraction        | 0.0956       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0151      |\n",
      "|    value_loss           | 38.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018098071 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.89        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 14.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008440407 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.696       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 5.49        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008411965 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.69        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0082     |\n",
      "|    value_loss           | 5.74        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=483.00 +/- 23.01\n",
      "Episode length: 483.00 +/- 23.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | 483         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008050531 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.466      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 4.98        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 483.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x24995c44860>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:26:11.045357Z",
     "start_time": "2025-01-12T10:26:11.040358Z"
    }
   },
   "cell_type": "code",
   "source": "env.render()",
   "id": "85344fa517260f06",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:13:30.265341Z",
     "start_time": "2025-01-12T10:13:30.262116Z"
    }
   },
   "cell_type": "code",
   "source": "from stable_baselines3 import DQN",
   "id": "bc431e91333b6c6c",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)",
   "id": "b7cad9b1b4cb513e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T02:09:49.862856Z",
     "start_time": "2025-01-14T02:09:49.538974Z"
    }
   },
   "cell_type": "code",
   "source": "model.learn(total_timesteps=20000, callback=eval_callback)",
   "id": "e33347468000e43",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model\u001B[38;5;241m.\u001B[39mlearn(total_timesteps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20000\u001B[39m, callback\u001B[38;5;241m=\u001B[39meval_callback)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d1df8c9b98eaf791"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
